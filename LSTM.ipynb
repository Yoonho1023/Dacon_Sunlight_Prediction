{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LSTM.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"BvtviskSG11T"},"source":["#내가 하는 태양광"]},{"cell_type":"code","metadata":{"id":"O_n9k1vyJYAY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"error","timestamp":1610872318049,"user_tz":-540,"elapsed":4724,"user":{"displayName":"장윤호","photoUrl":"","userId":"16970831253759087565"}},"outputId":"dfcaa982-6af2-4309-8f5b-7380d4b7d932"},"source":["# 패키지 인포트!!!\n","import pandas as pd\n","import numpy as np\n","import warnings\n","warnings.filterwarnings(action='ignore')\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset,DataLoader\n","import torch.optim as optim\n","torch.manual_seed(1015)\n","# define 'device' to upload tensor in gpu\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  # CPU, GPU 선택해줌\n","\n","import pandas as pd\n","import numpy as np\n","import os\n","import glob\n","import random\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","#pytorch 라이브러리 임포트\n","import torch\n","from torch.autograd import Variable\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","#scikit-learn 라이브러리\n","from sklearn.datasets import load_digits\n","from sklearn import datasets, model_selection\n","\n","#pandas 라이브러리 임포트\n","import pandas as pd\n","\n","#matplotlib 라이브러리 임포트\n","from matplotlib import pyplot as plt\n","from matplotlib import cm\n","%matplotlib inline #이미지를 노트북 안에 출력하도록 하는것"],"execution_count":null,"outputs":[{"output_type":"stream","text":["UsageError: unrecognized arguments: #이미지를 노트북 안에 출력하도록 하는것\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"J886t8UpKsfc"},"source":["> 데이터 확인\n","* https://dacon.io/competitions/official/235683/data/\n","* 데이터 불러오기 이후 순차 실행\n","* 제일 핵심 요소만"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i1M_-xWDGtTH","executionInfo":{"status":"ok","timestamp":1610872341889,"user_tz":-540,"elapsed":22484,"user":{"displayName":"장윤호","photoUrl":"","userId":"16970831253759087565"}},"outputId":"74c9f972-74df-44b3-d02a-71d8b410641f"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qt6xwxyaG8uE"},"source":["train_raw = pd.read_csv('/content/drive/MyDrive/Dacon/태양광 발전량 예측 AI 경진대회/train/train.csv')\n","train= train_raw"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s9vk6ZeXG8dg"},"source":["submission = pd.read_csv('/content/drive/MyDrive/Dacon/태양광 발전량 예측 AI 경진대회/sample_submission.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"fB-rja0RJmZ0","executionInfo":{"status":"ok","timestamp":1610872480544,"user_tz":-540,"elapsed":830,"user":{"displayName":"장윤호","photoUrl":"","userId":"16970831253759087565"}},"outputId":"f6543d22-dcdf-4ff4-b0ea-d90571c7e37d"},"source":["def preprocess_data(data, is_train=True):\n","    \n","    temp = data.copy()\n","    temp = temp[['Hour', 'TARGET', 'DHI', 'DNI', 'WS', 'RH', 'T']]\n","\n","    if is_train==True:          \n","    \n","        temp['Target1'] = temp['TARGET'].shift(-48).fillna(method='ffill')\n","        temp['Target2'] = temp['TARGET'].shift(-48*2).fillna(method='ffill')\n","        temp = temp.dropna()\n","        \n","        return temp.iloc[:-96] # 왜 마지막 2일을 빼고 학습?\n","\n","    elif is_train==False:\n","       \n","        temp = temp[['Hour', 'TARGET', 'DHI', 'DNI', 'WS', 'RH', 'T']]\n","                              \n","        return temp.iloc[-48:, :] # 왜 마지막 날들만 빼서 데이터를 모으지?\n","\n","\n","df_train = preprocess_data(train)\n","df_train.iloc[:48]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Hour</th>\n","      <th>TARGET</th>\n","      <th>DHI</th>\n","      <th>DNI</th>\n","      <th>WS</th>\n","      <th>RH</th>\n","      <th>T</th>\n","      <th>Target1</th>\n","      <th>Target2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1.5</td>\n","      <td>69.08</td>\n","      <td>-12</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1.5</td>\n","      <td>69.06</td>\n","      <td>-12</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1.6</td>\n","      <td>71.78</td>\n","      <td>-12</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1.6</td>\n","      <td>71.75</td>\n","      <td>-12</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1.6</td>\n","      <td>75.20</td>\n","      <td>-12</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>2</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1.5</td>\n","      <td>69.29</td>\n","      <td>-11</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>3</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1.5</td>\n","      <td>72.56</td>\n","      <td>-11</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>3</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1.4</td>\n","      <td>72.55</td>\n","      <td>-11</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>4</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1.3</td>\n","      <td>74.62</td>\n","      <td>-11</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>4</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1.3</td>\n","      <td>74.61</td>\n","      <td>-11</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>5</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1.3</td>\n","      <td>73.74</td>\n","      <td>-11</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>5</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1.3</td>\n","      <td>73.73</td>\n","      <td>-11</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>6</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1.4</td>\n","      <td>72.22</td>\n","      <td>-12</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>6</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1.4</td>\n","      <td>72.22</td>\n","      <td>-12</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>7</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1.4</td>\n","      <td>70.27</td>\n","      <td>-12</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>7</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1.6</td>\n","      <td>64.83</td>\n","      <td>-10</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>8</td>\n","      <td>7.039287</td>\n","      <td>29</td>\n","      <td>494</td>\n","      <td>1.8</td>\n","      <td>65.45</td>\n","      <td>-9</td>\n","      <td>7.133144</td>\n","      <td>0.750808</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>8</td>\n","      <td>5.912871</td>\n","      <td>61</td>\n","      <td>7</td>\n","      <td>1.9</td>\n","      <td>55.90</td>\n","      <td>-7</td>\n","      <td>14.922796</td>\n","      <td>1.689299</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>9</td>\n","      <td>22.337268</td>\n","      <td>58</td>\n","      <td>743</td>\n","      <td>2.1</td>\n","      <td>57.39</td>\n","      <td>-6</td>\n","      <td>22.806037</td>\n","      <td>5.161690</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>9</td>\n","      <td>29.469529</td>\n","      <td>67</td>\n","      <td>811</td>\n","      <td>1.9</td>\n","      <td>53.15</td>\n","      <td>-4</td>\n","      <td>2.158549</td>\n","      <td>4.973992</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>10</td>\n","      <td>25.339762</td>\n","      <td>138</td>\n","      <td>368</td>\n","      <td>1.8</td>\n","      <td>55.99</td>\n","      <td>-3</td>\n","      <td>36.225679</td>\n","      <td>16.517408</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>10</td>\n","      <td>25.152060</td>\n","      <td>178</td>\n","      <td>224</td>\n","      <td>1.9</td>\n","      <td>55.97</td>\n","      <td>-3</td>\n","      <td>41.386914</td>\n","      <td>9.760179</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>11</td>\n","      <td>28.718397</td>\n","      <td>193</td>\n","      <td>261</td>\n","      <td>2.0</td>\n","      <td>58.43</td>\n","      <td>-3</td>\n","      <td>45.140829</td>\n","      <td>7.601678</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>11</td>\n","      <td>33.129393</td>\n","      <td>189</td>\n","      <td>364</td>\n","      <td>2.3</td>\n","      <td>58.41</td>\n","      <td>-3</td>\n","      <td>47.580874</td>\n","      <td>10.980202</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>12</td>\n","      <td>19.427151</td>\n","      <td>190</td>\n","      <td>37</td>\n","      <td>2.6</td>\n","      <td>59.19</td>\n","      <td>-3</td>\n","      <td>48.612666</td>\n","      <td>17.361857</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>12</td>\n","      <td>25.715166</td>\n","      <td>213</td>\n","      <td>133</td>\n","      <td>2.9</td>\n","      <td>59.18</td>\n","      <td>-3</td>\n","      <td>48.143432</td>\n","      <td>4.880090</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>13</td>\n","      <td>24.589225</td>\n","      <td>211</td>\n","      <td>114</td>\n","      <td>3.2</td>\n","      <td>63.27</td>\n","      <td>-4</td>\n","      <td>46.172141</td>\n","      <td>13.326545</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>13</td>\n","      <td>21.304405</td>\n","      <td>185</td>\n","      <td>101</td>\n","      <td>3.1</td>\n","      <td>63.27</td>\n","      <td>-4</td>\n","      <td>42.794162</td>\n","      <td>9.478740</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>14</td>\n","      <td>11.731500</td>\n","      <td>124</td>\n","      <td>2</td>\n","      <td>3.0</td>\n","      <td>62.84</td>\n","      <td>-4</td>\n","      <td>38.195666</td>\n","      <td>4.504797</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>14</td>\n","      <td>14.734764</td>\n","      <td>135</td>\n","      <td>69</td>\n","      <td>2.7</td>\n","      <td>62.85</td>\n","      <td>-4</td>\n","      <td>32.283670</td>\n","      <td>2.627827</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>15</td>\n","      <td>5.818888</td>\n","      <td>62</td>\n","      <td>0</td>\n","      <td>2.5</td>\n","      <td>68.55</td>\n","      <td>-5</td>\n","      <td>25.432775</td>\n","      <td>14.265504</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>15</td>\n","      <td>7.602096</td>\n","      <td>73</td>\n","      <td>39</td>\n","      <td>2.2</td>\n","      <td>68.55</td>\n","      <td>-5</td>\n","      <td>17.737444</td>\n","      <td>16.611987</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>16</td>\n","      <td>4.035725</td>\n","      <td>41</td>\n","      <td>11</td>\n","      <td>2.0</td>\n","      <td>70.27</td>\n","      <td>-6</td>\n","      <td>9.854244</td>\n","      <td>9.010089</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>16</td>\n","      <td>0.938541</td>\n","      <td>10</td>\n","      <td>0</td>\n","      <td>2.0</td>\n","      <td>70.28</td>\n","      <td>-6</td>\n","      <td>2.627827</td>\n","      <td>2.440259</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>17</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2.0</td>\n","      <td>71.33</td>\n","      <td>-7</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>17</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2.0</td>\n","      <td>71.35</td>\n","      <td>-7</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>18</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2.1</td>\n","      <td>76.43</td>\n","      <td>-8</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>18</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2.2</td>\n","      <td>76.44</td>\n","      <td>-8</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>19</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2.3</td>\n","      <td>76.72</td>\n","      <td>-8</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>19</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2.2</td>\n","      <td>76.72</td>\n","      <td>-8</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>20</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2.2</td>\n","      <td>77.51</td>\n","      <td>-8</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>20</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2.0</td>\n","      <td>77.51</td>\n","      <td>-8</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>42</th>\n","      <td>21</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1.9</td>\n","      <td>83.46</td>\n","      <td>-9</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>21</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1.8</td>\n","      <td>83.46</td>\n","      <td>-9</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>44</th>\n","      <td>22</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1.8</td>\n","      <td>83.36</td>\n","      <td>-9</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>45</th>\n","      <td>22</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1.7</td>\n","      <td>83.36</td>\n","      <td>-9</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>46</th>\n","      <td>23</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1.7</td>\n","      <td>90.86</td>\n","      <td>-10</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>47</th>\n","      <td>23</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1.6</td>\n","      <td>90.85</td>\n","      <td>-10</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    Hour     TARGET  DHI  DNI   WS     RH   T    Target1    Target2\n","0      0   0.000000    0    0  1.5  69.08 -12   0.000000   0.000000\n","1      0   0.000000    0    0  1.5  69.06 -12   0.000000   0.000000\n","2      1   0.000000    0    0  1.6  71.78 -12   0.000000   0.000000\n","3      1   0.000000    0    0  1.6  71.75 -12   0.000000   0.000000\n","4      2   0.000000    0    0  1.6  75.20 -12   0.000000   0.000000\n","5      2   0.000000    0    0  1.5  69.29 -11   0.000000   0.000000\n","6      3   0.000000    0    0  1.5  72.56 -11   0.000000   0.000000\n","7      3   0.000000    0    0  1.4  72.55 -11   0.000000   0.000000\n","8      4   0.000000    0    0  1.3  74.62 -11   0.000000   0.000000\n","9      4   0.000000    0    0  1.3  74.61 -11   0.000000   0.000000\n","10     5   0.000000    0    0  1.3  73.74 -11   0.000000   0.000000\n","11     5   0.000000    0    0  1.3  73.73 -11   0.000000   0.000000\n","12     6   0.000000    0    0  1.4  72.22 -12   0.000000   0.000000\n","13     6   0.000000    0    0  1.4  72.22 -12   0.000000   0.000000\n","14     7   0.000000    0    0  1.4  70.27 -12   0.000000   0.000000\n","15     7   0.000000    0    0  1.6  64.83 -10   0.000000   0.000000\n","16     8   7.039287   29  494  1.8  65.45  -9   7.133144   0.750808\n","17     8   5.912871   61    7  1.9  55.90  -7  14.922796   1.689299\n","18     9  22.337268   58  743  2.1  57.39  -6  22.806037   5.161690\n","19     9  29.469529   67  811  1.9  53.15  -4   2.158549   4.973992\n","20    10  25.339762  138  368  1.8  55.99  -3  36.225679  16.517408\n","21    10  25.152060  178  224  1.9  55.97  -3  41.386914   9.760179\n","22    11  28.718397  193  261  2.0  58.43  -3  45.140829   7.601678\n","23    11  33.129393  189  364  2.3  58.41  -3  47.580874  10.980202\n","24    12  19.427151  190   37  2.6  59.19  -3  48.612666  17.361857\n","25    12  25.715166  213  133  2.9  59.18  -3  48.143432   4.880090\n","26    13  24.589225  211  114  3.2  63.27  -4  46.172141  13.326545\n","27    13  21.304405  185  101  3.1  63.27  -4  42.794162   9.478740\n","28    14  11.731500  124    2  3.0  62.84  -4  38.195666   4.504797\n","29    14  14.734764  135   69  2.7  62.85  -4  32.283670   2.627827\n","30    15   5.818888   62    0  2.5  68.55  -5  25.432775  14.265504\n","31    15   7.602096   73   39  2.2  68.55  -5  17.737444  16.611987\n","32    16   4.035725   41   11  2.0  70.27  -6   9.854244   9.010089\n","33    16   0.938541   10    0  2.0  70.28  -6   2.627827   2.440259\n","34    17   0.000000    0    0  2.0  71.33  -7   0.000000   0.000000\n","35    17   0.000000    0    0  2.0  71.35  -7   0.000000   0.000000\n","36    18   0.000000    0    0  2.1  76.43  -8   0.000000   0.000000\n","37    18   0.000000    0    0  2.2  76.44  -8   0.000000   0.000000\n","38    19   0.000000    0    0  2.3  76.72  -8   0.000000   0.000000\n","39    19   0.000000    0    0  2.2  76.72  -8   0.000000   0.000000\n","40    20   0.000000    0    0  2.2  77.51  -8   0.000000   0.000000\n","41    20   0.000000    0    0  2.0  77.51  -8   0.000000   0.000000\n","42    21   0.000000    0    0  1.9  83.46  -9   0.000000   0.000000\n","43    21   0.000000    0    0  1.8  83.46  -9   0.000000   0.000000\n","44    22   0.000000    0    0  1.8  83.36  -9   0.000000   0.000000\n","45    22   0.000000    0    0  1.7  83.36  -9   0.000000   0.000000\n","46    23   0.000000    0    0  1.7  90.86 -10   0.000000   0.000000\n","47    23   0.000000    0    0  1.6  90.85 -10   0.000000   0.000000"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"z0-MYtidJbmz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610872591999,"user_tz":-540,"elapsed":790,"user":{"displayName":"장윤호","photoUrl":"","userId":"16970831253759087565"}},"outputId":"74903376-833c-4532-b787-747f72ad5b50"},"source":["train1= df_train\n","train1.iloc[:,1:8].min()\n","train1.iloc[:,1:8].max()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TARGET       99.913939\n","DHI         528.000000\n","DNI        1059.000000\n","WS           12.000000\n","RH          100.000000\n","T            35.000000\n","Target1      99.913939\n","dtype: float64"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"bqQUN59TG8bc"},"source":["# scaling 1\n","mini = train1.iloc[:,1:8].min()\n","size = train1.iloc[:,1:8].max() - train1.iloc[:,1:8].min()\n","train1.iloc[:,1:8] = (train1.iloc[:,1:8] -  mini) / size    # 민맥스 그 뭐냐 그 정규화임!!\n","\n","input_window = 8       # 들어가는 수, 훈련 , 24시간 10일들어가서\n","output_window = 1       # 나오는 수, 테스트 또는 결과 , 24시간을 예측해봐\n","\n","# 행렬을 먼저 만듬\n","window_x = np.zeros((train1.shape[0] - (input_window + output_window), input_window, 8))  # 데이터 전체에서 (들어가는 수랑 나오는 수)를 뺌(이게 for문 돌아갈때 마지막은 빼야하니까),  맨뒤에 4는 4개의 컬럼(사용자, 세션, 신규방문자, 페이지뷰),    (배치 사이즈, 시퀀스 길이, input 차원)\n","window_y = np.zeros((train1.shape[0] - (input_window + output_window), output_window, 8))\n","\n","\n","# 데이터 값을 넣음\n","for start in range(train1.shape[0] - (input_window + output_window)):  #데이터의 수만큼 돌아감, 하루씩 늘려가며 데이터 셋을 생성 다르게 말하면 데이터가 겹침!!!!\n","    end = start + input_window    # 데이터의 끝은 시작값과 들어가는 수, 훈련을 168행개\n","    window_x[start,:, :] = train1.iloc[start : end                , 1: ].values   # 한 데이터의 길이(배치 사이즈), 한데이터에 들어가는 데이터의 행 수(시퀸스 길이), 컬럼의 개수(인풋 차원) , 빈 행렬에 값을 넣기\n","    window_y[start,:, :] = train1.iloc[end   : end + output_window, 1: ].values   # 앞에 x에서 끝나는 뒤에, 24행개만 들어감, 그러니까 x랑 데이터가 겹칠 수 있으"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WyVPzXp2G8ZI","executionInfo":{"status":"ok","timestamp":1610872829698,"user_tz":-540,"elapsed":989,"user":{"displayName":"장윤호","photoUrl":"","userId":"16970831253759087565"}},"outputId":"600f0681-9b2c-4ca9-dddd-595f83729342"},"source":["class LSTM(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(LSTM, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.lstm = nn.LSTM(input_size = input_size,\n","                            hidden_size = hidden_size,\n","                            batch_first=True)\n","        \n","        self.hidden_lstm = nn.LSTM(input_size = input_size,\n","                                   hidden_size = hidden_size,\n","                                   batch_first=True)\n","        \n","        self.time_fc = nn.Linear(hidden_size, 8)\n","    \n","    def forward(self, x_time):\n","    \n","        out_time, _ = self.lstm(x_time)\n","        out_time, _ = self.hidden_lstm(out_time)\n","        out_time, _ = self.hidden_lstm(out_time)\n","        out_time, _ = self.hidden_lstm(out_time)\n","        \n","        \n","        out_time = self.time_fc(out_time[:,-7:, :])\n","        \n","        return out_time.view(-1,7,4)\n","    \n","model = LSTM(input_size = 8, hidden_size = 8)\n","model"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LSTM(\n","  (lstm): LSTM(8, 8, batch_first=True)\n","  (hidden_lstm): LSTM(8, 8, batch_first=True)\n","  (time_fc): Linear(in_features=8, out_features=8, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w1zX499oLA9f","executionInfo":{"status":"ok","timestamp":1610872807850,"user_tz":-540,"elapsed":771,"user":{"displayName":"장윤호","photoUrl":"","userId":"16970831253759087565"}},"outputId":"6de1b729-ea8f-481d-9f8c-c158bee2174f"},"source":["window_x.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([52455, 8, 8])"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":394},"id":"C6llqMZPG8Wy","executionInfo":{"status":"error","timestamp":1610872835438,"user_tz":-540,"elapsed":2166,"user":{"displayName":"장윤호","photoUrl":"","userId":"16970831253759087565"}},"outputId":"a8995006-d90e-4911-acf4-716a7808782c"},"source":["# Model학습\n","\n","window_x = torch.tensor(window_x).float().to(device)\n","window_y = torch.tensor(window_y).float().to(device)\n","\n","# Train model\n","optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)  # 1e-2\n","criterion = nn.MSELoss(size_average = True)\n","num_epochs  = 2000\n","train_error = []\n","\n","for t in range(num_epochs):\n","    train_pred = model(window_x)\n","    loss = criterion(train_pred, window_y) ### trend\n","    train_error.append(loss)\n","    \n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","    if t % 100 == 0 and t !=0:\n","        print(f\"{t} Epochs train MSE: {loss.item():1.5f}\")"],"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-0df2a6eee06d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtrain_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_y\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m### trend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mtrain_error\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2657\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m     \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (8) at non-singleton dimension 2"]}]},{"cell_type":"code","metadata":{"id":"ocXmCYUHG8UP"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uThjtS4BG8Ry"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nk3eeKf3G8PK"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c9XYLj6iG8Ft"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2xtex3KTGzTm"},"source":["# 성민이가 준 파일"]},{"cell_type":"code","metadata":{"id":"uvZud-vxG5oa"},"source":["# 패키지 인포트!!!\n","import pandas as pd\n","import numpy as np\n","import warnings\n","warnings.filterwarnings(action='ignore')\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset,DataLoader\n","import torch.optim as optim\n","torch.manual_seed(1015)\n","# define 'device' to upload tensor in gpu\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  # CPU, GPU 선택해줌"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dLhODLwHJnuF"},"source":["# 데이터 불러오기\n","train = pd.read_csv(\"../../1. 데이터/open_data/train.csv\", encoding = 'euc-kr')\n","train['DateTime'] = pd.to_datetime(train.DateTime)\n","train['date'] = train.DateTime.dt.date\n","train1  = train.groupby('date').sum().reset_index()  # 시간을 날짜별로 합쳐서 생성"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kY6D4n2CJnwp"},"source":["# 한번만!!!!!!!!!!!! 계속하면 0이됨\n","# 이거로 변경하자\n","\n","# scaling\n","mini = train1.iloc[:,1:].min()\n","size = train1.iloc[:,1:].max() - train1.iloc[:,1:].min()\n","train1.iloc[:,1:] = (train1.iloc[:,1:] -  mini) / size    # 민맥스 그 뭐냐 그 정규화임!!\n","\n","input_window = 10       # 들어가는 수, 훈련 , 24시간 10일들어가서\n","output_window = 1       # 나오는 수, 테스트 또는 결과 , 24시간을 예측해봐\n","\n","# 행렬을 먼저 만듬\n","window_x = np.zeros((train1.shape[0] - (input_window + output_window), input_window, 4))  # 데이터 전체에서 (들어가는 수랑 나오는 수)를 뺌(이게 for문 돌아갈때 마지막은 빼야하니까),  맨뒤에 4는 4개의 컬럼(사용자, 세션, 신규방문자, 페이지뷰),    (배치 사이즈, 시퀀스 길이, input 차원)\n","window_y = np.zeros((train1.shape[0] - (input_window + output_window), output_window, 4))\n","\n","\n","# 데이터 값을 넣음\n","for start in range(train1.shape[0] - (input_window + output_window)):  #데이터의 수만큼 돌아감, 하루씩 늘려가며 데이터 셋을 생성 다르게 말하면 데이터가 겹침!!!!\n","    end = start + input_window    # 데이터의 끝은 시작값과 들어가는 수, 훈련을 168행개\n","    window_x[start,:, :] = train1.iloc[start : end                , 1: ].values   # 한 데이터의 길이(배치 사이즈), 한데이터에 들어가는 데이터의 행 수(시퀸스 길이), 컬럼의 개수(인풋 차원) , 빈 행렬에 값을 넣기\n","    window_y[start,:, :] = train1.iloc[end   : end + output_window, 1: ].values   # 앞에 x에서 끝나는 뒤에, 24행개만 들어감, 그러니까 x랑 데이터가 겹칠 수 있으"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oz-pGu4BJ9ym"},"source":["# Model 생성\n","# 이걸로 가지고 놀꺼임\n","\n","class LSTM(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(LSTM, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.lstm = nn.LSTM(input_size = input_size,\n","                            hidden_size = hidden_size,\n","                            batch_first=True)\n","        self.hidden_lstm = nn.LSTM(input_size = hidden_size,\n","                                   hidden_size = hidden_size,\n","                                   batch_first=True)\n","        \n","        self.time_fc = nn.Linear(hidden_size, 4)\n","    \n","    def forward(self, x_time):\n","    \n","        out_time, _ = self.lstm(x_time)\n","        out_time, _ = self.hidden_lstm(out_time)\n","        out_time, _ = self.hidden_lstm(out_time)\n","        out_time, _ = self.hidden_lstm(out_time)\n","        out_time, _ = self.hidden_lstm(out_time)\n","        \n","        \n","        out_time = self.time_fc(out_time[:,-1:, :])\n","        \n","        return out_time.view(-1,1,4)\n","\n","  \n","model = LSTM(input_size = 4, hidden_size = 15).to(device)  # 인풋은 들어가는 컬럼의 수를, 히든은 들어가는 중간 시퀸스 길이(날짜 혹은 들어가는 사이즈)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Lzkx9nEJ9tZ"},"source":["# Model학습\n","\n","window_x = torch.tensor(window_x).float().to(device)\n","window_y = torch.tensor(window_y).float().to(device)\n","\n","# Train model\n","optimizer = torch.optim.Adam(model.parameters(), lr = 0.004)  # 1e-2\n","criterion = nn.MSELoss(size_average = True)\n","num_epochs  = 2000\n","train_error = []\n","\n","for t in range(num_epochs):\n","    train_pred = model(window_x)\n","    loss = criterion(train_pred, window_y) ### trend\n","    train_error.append(loss)\n","    \n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","    if t % 100 == 0 and t !=0:\n","        print(f\"{t} Epochs train MSE: {loss.item():1.5f}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"41FdWeXNLFNU"},"source":["> 정리\n","* 딥러닝 시계열 계열을 돌릴때는 x와 y를 (데이터 수, 사용할 날짜, 컬럼 수)로 해야함!!\n","* 학습 시, 손실함수는 MSELoss"]},{"cell_type":"code","metadata":{"id":"TTn2reTLJnzN"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wxKYi3gzJn1x"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5KPE4WZmJn4L"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qnQ3h4shJn60"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FUOVmGVZJn9g"},"source":[""],"execution_count":null,"outputs":[]}]}